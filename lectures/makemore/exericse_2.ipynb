{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ba2c56-4482-45e2-88fa-503e404ad4d9",
   "metadata": {},
   "source": [
    "## exercise 2\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047bdfe-a15f-40ad-ad7c-a7c4438b44ed",
   "metadata": {},
   "source": [
    "### train on the train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fb8e4aa-5a77-4e09-a5b9-a7b84448efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "# 利用set能够去重的功能,找出文本中有哪些字符\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# 建立index->character之间的互相转换,额外加上填充字符'.'\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42502342-9813-4d0f-97a0-557167846da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7221f95a-fa9f-48ec-b213-96f6bcf2c5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd844d32-2561-4bbb-922f-ba42145328d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigram_dataset(words):\n",
    "    # create the dataset\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "      chs = ['.']+ ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1,ix2])\n",
    "        ys.append(ix3)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    num = int(xs.nelement()/2)\n",
    "    print('number of examples: ', num)\n",
    "    return (xs,ys,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d008360-a8a5-489c-90d9-2e63fdcafa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  182441\n",
      "number of examples:  22902\n",
      "number of examples:  22803\n",
      "总单词数: 32033\n",
      "训练集单词数: 25626 -> 训练样本数: 182441\n",
      "开发集单词数: 3203 -> 开发样本数: 22902\n",
      "测试集单词数: 3204 -> 测试样本数: 22803\n"
     ]
    }
   ],
   "source": [
    "# --- E02 的核心实现 ---\n",
    "# 1. 设置一个随机种子，保证每次划分都一样（可复现）\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 2. 按“单词”划分 80/10/10\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "train_words = words[:n1]\n",
    "dev_words   = words[n1:n2]\n",
    "test_words  = words[n2:]\n",
    "\n",
    "# 3. 创建三个完全分离 的数据集\n",
    "xs_train, ys_train, num_train = create_trigram_dataset(train_words)\n",
    "xs_dev,   ys_dev, num_dev = create_trigram_dataset(dev_words)\n",
    "xs_test,  ys_test, num_test = create_trigram_dataset(test_words)\n",
    "\n",
    "# 5. 打印结果，确认一下\n",
    "print(f\"总单词数: {len(words)}\")\n",
    "print(f\"训练集单词数: {len(train_words)} -> 训练样本数: {len(xs_train)}\")\n",
    "print(f\"开发集单词数: {len(dev_words)} -> 开发样本数: {len(xs_dev)}\")\n",
    "print(f\"测试集单词数: {len(test_words)} -> 测试样本数: {len(xs_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c07a010-8018-43aa-9a3d-607189b084e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "import torch.nn.functional as F\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc74116-c495-41d0-ac6d-89a55eae44d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9720e739-0337-408c-bfa3-4c416eb522b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for k in range(1000):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs_train, num_classes=27).float().view(-1,54) # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num_train), ys_train].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732499ed-face-4af0-b261-0606b8584245",
   "metadata": {},
   "source": [
    "### evaluate on dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c02652c0-1ba5-4cd9-9c50-ba9ef4e72884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.357011079788208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# forward pass\n",
    "xenc = F.one_hot(xs_dev, num_classes=27).float().view(-1,54) # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss_dev = -probs[torch.arange(num_dev), ys_dev].log().mean() + 0.01*(W**2).mean()\n",
    "print(loss_dev.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3e5b5-5f13-42ba-8e33-4b3bb70dbed5",
   "metadata": {},
   "source": [
    "### evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3cb8cd7a-ccc3-4baa-ae63-b57fa0c1f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.356856107711792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# forward pass\n",
    "xenc = F.one_hot(xs_test, num_classes=27).float().view(-1,54) # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss_dev = -probs[torch.arange(num_test), ys_test].log().mean() + 0.01*(W**2).mean()\n",
    "print(loss_dev.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6c2b0-c659-4b2d-8727-3a430f0ba399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59e3508d-b08b-4b7b-af68-8f5b862f77a5",
   "metadata": {},
   "source": [
    "### remodel the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2814d09-8a66-41ae-9222-e7653536ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigram_dataset(words):\n",
    "    # create the dataset\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "      chs = ['.']+ ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        ix_pair = ix1 * 27 + ix2\n",
    "        xs.append(ix_pair)\n",
    "        ys.append(ix3)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    num = ys.nelement()\n",
    "    print('number of examples: ', num)\n",
    "    return (xs,ys,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d33b6bcd-e0d5-4966-80c3-c779a6ef0293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  182484\n",
      "number of examples:  22869\n",
      "number of examples:  22793\n",
      "总单词数: 32033\n",
      "训练集单词数: 25626 -> 训练样本数: 182484\n",
      "开发集单词数: 3203 -> 开发样本数: 22869\n",
      "测试集单词数: 3204 -> 测试样本数: 22793\n"
     ]
    }
   ],
   "source": [
    "# --- E02 的核心实现 ---\n",
    "# 1. 设置一个随机种子，保证每次划分都一样（可复现）\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 2. 按“单词”划分 80/10/10\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "train_words = words[:n1]\n",
    "dev_words   = words[n1:n2]\n",
    "test_words  = words[n2:]\n",
    "\n",
    "# 3. 创建三个完全分离 的数据集\n",
    "xs_train, ys_train, num_train = create_trigram_dataset(train_words)\n",
    "xs_dev,   ys_dev, num_dev = create_trigram_dataset(dev_words)\n",
    "xs_test,  ys_test, num_test = create_trigram_dataset(test_words)\n",
    "\n",
    "# 5. 打印结果，确认一下\n",
    "print(f\"总单词数: {len(words)}\")\n",
    "print(f\"训练集单词数: {len(train_words)} -> 训练样本数: {len(xs_train)}\")\n",
    "print(f\"开发集单词数: {len(dev_words)} -> 开发样本数: {len(xs_dev)}\")\n",
    "print(f\"测试集单词数: {len(test_words)} -> 测试样本数: {len(xs_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "973d6ec5-ac02-42e4-a73b-0794d1411c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "import torch.nn.functional as F\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "175fe7a6-2afc-4923-afd0-d98866f5d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.978973627090454\n",
      "2.9625449180603027\n",
      "2.9468915462493896\n",
      "2.931957483291626\n",
      "2.9176950454711914\n",
      "2.9040608406066895\n",
      "2.8910152912139893\n",
      "2.8785219192504883\n",
      "2.8665482997894287\n",
      "2.8550643920898438\n",
      "2.844041347503662\n",
      "2.83345365524292\n",
      "2.8232765197753906\n",
      "2.8134868144989014\n",
      "2.804063558578491\n",
      "2.7949864864349365\n",
      "2.7862355709075928\n",
      "2.7777936458587646\n",
      "2.7696430683135986\n",
      "2.761768102645874\n",
      "2.7541539669036865\n",
      "2.74678635597229\n",
      "2.739651918411255\n",
      "2.7327382564544678\n",
      "2.726034164428711\n",
      "2.7195286750793457\n",
      "2.7132112979888916\n",
      "2.7070729732513428\n",
      "2.7011048793792725\n",
      "2.695298910140991\n",
      "2.6896474361419678\n",
      "2.684143304824829\n",
      "2.678779363632202\n",
      "2.6735503673553467\n",
      "2.6684494018554688\n",
      "2.6634716987609863\n",
      "2.658612012863159\n",
      "2.653865337371826\n",
      "2.6492276191711426\n",
      "2.644693613052368\n",
      "2.640260696411133\n",
      "2.6359238624572754\n",
      "2.631680488586426\n",
      "2.6275265216827393\n",
      "2.6234593391418457\n",
      "2.619475841522217\n",
      "2.615572452545166\n",
      "2.6117472648620605\n",
      "2.607997179031372\n",
      "2.6043202877044678\n",
      "2.6007139682769775\n",
      "2.5971763134002686\n",
      "2.5937044620513916\n",
      "2.590297222137451\n",
      "2.586951971054077\n",
      "2.583667516708374\n",
      "2.580441474914551\n",
      "2.577272891998291\n",
      "2.574159622192383\n",
      "2.5711002349853516\n",
      "2.5680932998657227\n",
      "2.5651373863220215\n",
      "2.5622313022613525\n",
      "2.559373617172241\n",
      "2.5565624237060547\n",
      "2.553797721862793\n",
      "2.551077365875244\n",
      "2.54840087890625\n",
      "2.545766830444336\n",
      "2.5431742668151855\n",
      "2.5406219959259033\n",
      "2.538109540939331\n",
      "2.535635232925415\n",
      "2.533198833465576\n",
      "2.530799150466919\n",
      "2.528435707092285\n",
      "2.526106834411621\n",
      "2.5238125324249268\n",
      "2.5215516090393066\n",
      "2.5193233489990234\n",
      "2.517127513885498\n",
      "2.5149624347686768\n",
      "2.512829065322876\n",
      "2.5107247829437256\n",
      "2.508650302886963\n",
      "2.5066041946411133\n",
      "2.5045864582061768\n",
      "2.502596616744995\n",
      "2.5006332397460938\n",
      "2.498696804046631\n",
      "2.496786117553711\n",
      "2.494900703430176\n",
      "2.493039846420288\n",
      "2.491204023361206\n",
      "2.489391565322876\n",
      "2.487603187561035\n",
      "2.48583722114563\n",
      "2.4840941429138184\n",
      "2.482372999191284\n",
      "2.480673313140869\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs_train, num_classes=729).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num_train), ys_train].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a8705-ef3e-49e2-9b68-d5dc5bca2155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f88649-e2f0-4fd8-acab-c910a0f2ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f5e05-ad84-4fcc-9dd0-ce4d13afd82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab1bd5-3801-4b74-9583-353057a9ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exercise 3\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
